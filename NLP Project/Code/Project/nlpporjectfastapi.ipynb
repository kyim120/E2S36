{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1V3i6iD3rjIImdNUeqs8NWmt24qSyq6q8","authorship_tag":"ABX9TyNXziQfVU0EKGe8nvwRAw9t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install git+https://github.com/Breakthrough/PySceneDetect.git\n","!pip install scenedetect\n","!pip install scenedetect\n","!pip install deepface\n","!pip install torch torchvision torchaudio\n","!pip install transformers\n","!pip install opencv-python-headless\n","!pip install ultralytics\n","!pip install matplotlib\n"],"metadata":{"id":"teU0OR-d4E24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyngrok import ngrok, conf\n","\n","# Add this at the beginning of your run_server() function\n","def run_server():\n","    # Clean up any existing tunnels\n","    try:\n","        ngrok.kill()\n","    except:\n","        pass\n","\n","    # Set ngrok configuration\n","    conf.get_default().auth_token = \"your-ngrok-token\"\n","    conf.get_default().region = \"us\"  # or other preferred region\n","\n","    # Create tunnel with retry logic\n","    max_retries = 3\n","    for attempt in range(max_retries):\n","        try:\n","            public_url = ngrok.connect(8000, bind_tls=True).public_url\n","            print(f\" * Public URL: {public_url}\")\n","            break\n","        except Exception as e:\n","            if attempt == max_retries - 1:\n","                print(\"Failed to establish ngrok tunnel after multiple attempts\")\n","                print(\"You can still access the server locally at http://localhost:8000\")\n","                public_url = None\n","            else:\n","                print(f\"Retrying ngrok connection (attempt {attempt + 1})\")\n","                time.sleep(2)\n","                continue\n","\n","    nest_asyncio.apply()\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"],"metadata":{"id":"JmvrV5reVgTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7thdgtbB7wJX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f7d7e403-0172-4159-e76b-5ff62a59aa72"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.2)\n","Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.5)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.117)\n","Requirement already satisfied: deepface in /usr/local/lib/python3.11/dist-packages (0.0.93)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n","Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n","Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n","Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n","Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n","Requirement already satisfied: flask-cors>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.0.1)\n","Requirement already satisfied: mtcnn>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.0.0)\n","Requirement already satisfied: retina-face>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (0.0.17)\n","Requirement already satisfied: fire>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (0.7.0)\n","Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (23.0.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (3.0.1)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.9)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.13.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.15.0)\n","Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n","Requirement already satisfied: lz4>=4.3.3 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (4.4.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (5.29.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.71.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.7)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"," * Public URL: https://5cfe-34-145-190-158.ngrok-free.app\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:     Started server process [37384]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["INFO:     139.135.44.42:0 - \"GET / HTTP/1.1\" 200 OK\n","INFO:     139.135.44.42:0 - \"GET /static/img/ai-avatar.png HTTP/1.1\" 404 Not Found\n","INFO:     139.135.44.42:0 - \"GET /static/css/style.css HTTP/1.1\" 200 OK\n","INFO:     139.135.44.42:0 - \"GET /static/js/script.js HTTP/1.1\" 200 OK\n","INFO:     139.135.44.42:0 - \"GET /static/img/robot.png HTTP/1.1\" 200 OK\n","\n","image 1/1 /content/drive/MyDrive/Project/static/uploads/Screenshot 2025-04-07 224942.png: 608x640 1 person, 259.1ms\n","Speed: 5.6ms preprocess, 259.1ms inference, 2.5ms postprocess per image at shape (1, 3, 608, 640)\n","INFO:     139.135.44.42:0 - \"POST /upload HTTP/1.1\" 200 OK\n","INFO:     139.135.44.42:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n","INFO:     139.135.44.42:0 - \"GET /static/uploads/Screenshot%202025-04-07%20224942.png HTTP/1.1\" 200 OK\n","INFO:     139.135.44.42:0 - \"POST /process HTTP/1.1\" 200 OK\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2025-04-26T19:06:06+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-b912d0c0-7e4d-470f-8dfc-d099a275b292 acceptErr=\"failed to accept connection: Listener closed\"\n","WARNING:pyngrok.process.ngrok:t=2025-04-26T19:06:06+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8000-b912d0c0-7e4d-470f-8dfc-d099a275b292 err=\"failed to start tunnel: session closed\"\n"]}],"source":["# Install required packages\n","!pip install fastapi uvicorn python-multipart pyngrok nest-asyncio\n","!pip install torch transformers ultralytics deepface\n","!apt-get install -y libgl1-mesa-glx  # For OpenCV\n","\n","# Import libraries\n","from fastapi import FastAPI, UploadFile, File, Request, HTTPException\n","from fastapi.staticfiles import StaticFiles\n","from fastapi.templating import Jinja2Templates\n","from fastapi.responses import HTMLResponse, JSONResponse, FileResponse\n","from fastapi.middleware.cors import CORSMiddleware\n","import torch\n","import requests\n","import cv2\n","from PIL import Image\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","from ultralytics import YOLO\n","from deepface import DeepFace\n","import numpy as np\n","import time\n","import collections\n","import os\n","import uvicorn\n","from pyngrok import ngrok\n","import nest_asyncio\n","import shutil\n","from pathlib import Path\n","\n","# Initialize FastAPI app\n","app = FastAPI()\n","\n","# Allow CORS for frontend development\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","# Mount static files from Google Drive\n","drive_path = \"/content/drive/MyDrive/Project\"\n","app.mount(\"/static\", StaticFiles(directory=f\"{drive_path}/static\"), name=\"static\")\n","templates = Jinja2Templates(directory=drive_path)\n","\n","# üîë Replace with your OpenRouter API Key\n","API_KEY = \"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\"\n","\n","# ‚úÖ Global Model Initialization\n","blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16)\n","blip_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","blip_model.to(blip_device)\n","\n","yolo_model_large = YOLO(\"yolov8n.pt\")\n","yolo_model_small = YOLO(\"yolov8n.pt\")\n","\n","# Create uploads directory if not exists\n","os.makedirs(f\"{drive_path}/static/uploads\", exist_ok=True)\n","\n","def get_task_from_prompt(prompt):\n","    headers = {\n","        \"Authorization\": f\"Bearer {API_KEY}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    system_msg = (\n","        \"You're a strict classifier. ONLY respond with one of the following exact words:\\n\"\n","        \"image_captioning, scene_description, object_detection, object_counting, \"\n","        \"question_answering, emotion_detection, video_summarization, real_time_tracking.\\n\"\n","        \"No explanation. No extra text.\"\n","    )\n","    data = {\n","        \"model\": \"deepseek/deepseek-chat\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": system_msg},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0\n","    }\n","    try:\n","        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=data)\n","        result = response.json()\n","        return result['choices'][0]['message']['content'].strip().lower()\n","    except:\n","        return \"error\"\n","\n","# üñäÔ∏è Generic Chat\n","\n","def get_small_talk_reply(prompt):\n","    headers = {\n","        \"Authorization\": f\"Bearer {API_KEY}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    data = {\n","        \"model\": \"deepseek/deepseek-chat\",\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": \"You're a helpful and friendly assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        \"temperature\": 0.7\n","    }\n","    try:\n","        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=data)\n","        return response.json()['choices'][0]['message']['content'].strip()\n","    except:\n","        return \"‚ùå Error getting reply.\"\n","\n","# üè∑Ô∏è Image Captioning\n","def caption_image(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = blip_processor(images=image, return_tensors=\"pt\").to(blip_device, torch.float16)\n","    generated_ids = blip_model.generate(**inputs, max_new_tokens=50)\n","    return blip_processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n","\n","# üì¶ Object Detection\n","def detect_objects(image_path):\n","    results = yolo_model_large(image_path)\n","    classes = results[0].names\n","    class_ids = results[0].boxes.cls.tolist()\n","    return list(set(classes[int(cls_id)] for cls_id in class_ids))\n","\n","# üî¢ Object Counting\n","def count_objects(image_path):\n","    results = yolo_model_large(image_path)\n","    classes = results[0].names\n","    class_ids = results[0].boxes.cls.tolist()\n","    count_dict = {}\n","    for cls_id in class_ids:\n","        label = classes[int(cls_id)]\n","        count_dict[label] = count_dict.get(label, 0) + 1\n","    return count_dict\n","\n","# üòä Emotion Detection\n","def detect_emotion_np(image_np):\n","    try:\n","        result = DeepFace.analyze(img_path=image_np, actions=[\"emotion\"], enforce_detection=False)\n","        if isinstance(result, list):\n","            return [r[\"dominant_emotion\"] for r in result]\n","        else:\n","            return [result['dominant_emotion']]\n","    except:\n","        return []\n","\n","# üìù Scene Description\n","def describe_caption_with_deepseek(caption, objects):\n","    headers = {\n","        \"Authorization\": f\"Bearer {API_KEY}\",\n","        \"Content-Type\": \"application/json\"\n","    }\n","    prompt = f\"Describe the scene based on the following details:\\nCaption: {caption}\\nDetected Objects: {', '.join(objects)}\"\n","    data = {\n","        \"model\": \"deepseek/deepseek-chat\",\n","        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","        \"temperature\": 0.7\n","    }\n","    try:\n","        response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\", headers=headers, json=data)\n","        return response.json()['choices'][0]['message']['content'].strip()\n","    except:\n","        return \"‚ùå Error generating scene description.\"\n","\n","# üé© Video Summarization\n","def summarize_video(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        return \"‚ùå Could not open video.\"\n","    frame_samples = []\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    sample_interval = max(1, total_frames // 10)\n","    for i in range(0, total_frames, sample_interval):\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","        ret, frame = cap.read()\n","        if ret:\n","            frame_samples.append(frame)\n","    cap.release()\n","\n","    detected = collections.Counter()\n","    emotions = []\n","    for frame in frame_samples:\n","        results = yolo_model_large.predict(frame)\n","        boxes = results[0].boxes\n","        if boxes:\n","            class_ids = boxes.cls.tolist()\n","            class_names = [results[0].names[int(c)] for c in class_ids]\n","            detected.update(class_names)\n","        emotions.extend(detect_emotion_np(frame))\n","\n","    summary_prompt = f\"Summarize this video:\\nDetected objects: {dict(detected)}\\nEmotions: {collections.Counter(emotions)}\"\n","    summary = get_small_talk_reply(summary_prompt)\n","    print(\"\\nüé© Video Summary:\\n\", summary)\n","# üîÑ Real-Time Tracking\n","def track_objects_from_video(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"‚ùå Could not open video.\")\n","        return\n","\n","    output_path = \"output_tracked.avi\"\n","    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), 2.5,\n","                          (640, 360))  # Half speed & resized\n","\n","    frame_id = 0\n","    unique_objects = collections.defaultdict(set)\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        frame_id += 1\n","        if frame_id % 10 != 0:  # Process every 10th frame\n","            continue\n","\n","        frame = cv2.resize(frame, (640, 360))  # Resize for speed\n","        results = yolo_model_small.track(frame, persist=True)\n","        boxes = results[0].boxes\n","        if boxes:\n","            class_ids = boxes.cls.tolist()\n","            track_ids = boxes.id.tolist() if boxes.id is not None else []\n","            for box, cls_id, track_id in zip(boxes.xyxy, class_ids, track_ids):\n","                x1, y1, x2, y2 = map(int, box)\n","                class_name = results[0].names[int(cls_id)]\n","                unique_objects[class_name].add(int(track_id))\n","                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","                cv2.putText(frame, f\"{class_name} ID:{int(track_id)}\", (x1, y1 - 10),\n","                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n","\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","    object_summary = \"\\n\".join([f\"{cls}: {len(ids)}\" for cls, ids in unique_objects.items()])\n","    summary_prompt = f\"Summarize the video:\\nDetected objects:\\n{object_summary}\"\n","    summary = get_small_talk_reply(summary_prompt)\n","    print(\"\\nüß† AI Summary of the video:\\n\", summary)\n","    print(\"\\nüì¶ Final Unique Object Count:\\n\", object_summary)\n","    print(FileLink(output_path))\n","def upload_file():\n","    uploaded = files.upload()\n","    for filename in uploaded.keys():\n","        return filename\n","\n","\n","# API Endpoints\n","@app.post(\"/upload\")\n","async def upload_file(file: UploadFile = File(...)):\n","    try:\n","        # Save the uploaded file\n","        file_location = f\"{drive_path}/static/uploads/{file.filename}\"\n","        with open(file_location, \"wb+\") as file_object:\n","            shutil.copyfileobj(file.file, file_object)\n","\n","        is_video = file.filename.lower().endswith(('.mp4', '.avi', '.mov'))\n","\n","        # Process the file\n","        caption = caption_image(file_location) if not is_video else None\n","        objects = detect_objects(file_location) if not is_video else None\n","\n","        return {\n","            \"filename\": file.filename,\n","            \"filepath\": f\"/static/uploads/{file.filename}\",\n","            \"is_video\": is_video,\n","            \"caption\": caption,\n","            \"objects\": objects\n","        }\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=str(e))\n","\n","@app.post(\"/process\")\n","async def process_request(request_data: dict):\n","    try:\n","        prompt = request_data.get(\"prompt\", \"\").strip().lower()\n","        filepath = request_data.get(\"filepath\", \"\")\n","        is_video = request_data.get(\"is_video\", False)\n","        caption = request_data.get(\"caption\", \"\")\n","        objects = request_data.get(\"objects\", [])\n","\n","        # Handle small talk\n","        small_talk_phrases = [\"hi\", \"hello\", \"hey\", \"yo\", \"howdy\", \"sup\",\n","                             \"good morning\", \"good evening\", \"what's up\"]\n","        if prompt in small_talk_phrases:\n","            return {\n","                \"response\": \"ü§ñ Hey there! üëã\\n‚ú® By the way, I can help you with tasks like:\\n- üñºÔ∏è Captioning\\n- üì∑ Descriptions\\n- üîç Detection\\n- üî¢ Counting\\n- üòä Emotions\\n- ‚ùì Q&A\\n- üé¨ Summarization\\n- üîÑ Tracking\",\n","                \"type\": \"small_talk\"\n","            }\n","\n","        # Get task from prompt\n","        task = get_task_from_prompt(prompt)\n","\n","        if not task:\n","            return {\n","                \"response\": \"ü§ñ I'm here for anything you need! Feel free to ask me about an image or video task anytime.\\nüéØ I'm capable of:\\n- üñºÔ∏è Captioning\\n- üì∑ Descriptions\\n- üîç Detection\\n- üî¢ Counting\\n- üòä Emotions\\n- ‚ùì Q&A\\n- üé¨ Summarization\\n- üîÑ Tracking\",\n","                \"type\": \"info\"\n","            }\n","\n","        if task in [\"real_time_tracking\", \"video_summarization\"] and not is_video:\n","            return {\n","                \"response\": \"‚ùå This task requires a video file.\",\n","                \"type\": \"error\"\n","            }\n","        elif task in [\"image_captioning\", \"scene_description\", \"object_detection\",\n","                      \"object_counting\", \"emotion_detection\", \"question_answering\"] and is_video:\n","            return {\n","                \"response\": \"‚ùå This task requires an image file.\",\n","                \"type\": \"error\"\n","            }\n","\n","        # Process the task\n","        full_path = f\"{drive_path}{filepath}\"\n","        result = {}\n","        if task == \"image_captioning\":\n","            result[\"response\"] = f\"üñºÔ∏è Caption: {caption}\"\n","        elif task == \"scene_description\":\n","            description = describe_caption_with_deepseek(caption, objects)\n","            result[\"response\"] = f\"üìù Scene Description: {description}\"\n","        elif task == \"object_detection\":\n","            result[\"response\"] = f\"üì¶ Detected Objects: {objects}\"\n","        elif task == \"object_counting\":\n","            count = count_objects(full_path)\n","            result[\"response\"] = f\"üî¢ Object Count: {count}\"\n","        elif task == \"emotion_detection\":\n","            img = cv2.imread(full_path)\n","            emotions = detect_emotion_np(img)\n","            result[\"response\"] = f\"üòä Detected Emotions: {collections.Counter(emotions)}\"\n","        elif task == \"question_answering\":\n","            answer = get_small_talk_reply(f\"Based on the image caption: '{caption}', answer: {prompt}\")\n","            result[\"response\"] = f\"üí° Answer: {answer}\"\n","        elif task == \"real_time_tracking\":\n","            tracking_result = track_objects_from_video(full_path)\n","            result[\"response\"] = f\"üß† AI Summary of the video:\\n{tracking_result['summary']}\\n\\nüì¶ Final Unique Object Count:\\n{tracking_result['object_summary']}\"\n","            result[\"video_path\"] = tracking_result[\"video_path\"]\n","        elif task == \"video_summarization\":\n","            summary = summarize_video(full_path)\n","            result[\"response\"] = f\"üé© Video Summary:\\n{summary}\"\n","\n","        result[\"type\"] = \"task_response\"\n","        return result\n","\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=str(e))\n","\n","@app.get(\"/\", response_class=HTMLResponse)\n","async def serve_frontend(request: Request):\n","    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n","\n","# Run the FastAPI server with ngrok\n","def run_server():\n","    # Set up ngrok (replace with your auth token)\n","    ngrok.set_auth_token(\"2wH83sGp2f24MRnLJqg7y7j5Mgy_87wid7Apcni7rhTr76Bmt\")\n","    public_url = ngrok.connect(8000).public_url\n","    print(f\" * Public URL: {public_url}\")\n","\n","    # Configure FastAPI to run with uvicorn\n","    nest_asyncio.apply()\n","    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n","\n","# Run this to start the server\n","run_server()"]},{"cell_type":"code","source":[],"metadata":{"id":"COGycIjQ9sTu"},"execution_count":null,"outputs":[]}]}